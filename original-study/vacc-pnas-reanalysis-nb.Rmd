---
title: "PNAS re-analysis notebook"
author: "Derek Powell"
output: 
  html_notebook: 
    code_folding: hide
---

This is a notebook to re-analyze the data from the PNAS paper "Countering Antivaccination Attitudes" using more appropriate statistical methods, such as ordinal HLM and beta regression. These analyses are being conducted in anticipation of a follow-up replication study.


```{r, echo=FALSE, results='hide'}
library(tidyverse)
library(brms)
if (!require(betareg)) {
  install.packages("betareg")
}
library(betareg)
```

```{r}
vaccAll <- read.csv("vacc-hphh-pubdata.csv") %>% 
  as_tibble() %>% 
  mutate(Participant = seq(1:n())) %>%
  filter(EligibleToReturn==1)

vaccW <- vaccAll %>%
  mutate(pretest = PreTestVaccinationAttitude, posttest = PostTest.Vaccination.Attitude) %>%
  filter(Returned == 1, Excluded == 0) %>%
  mutate(Condition = relevel(Condition, ref="Control")) %>%
  rename(condition = Condition)

```


# Original Analysis (change scores)

```{r}
fit.change <- lm(change ~ condition, data = vaccW %>% mutate(change = posttest - pretest))
summary(fit.change)
```

## OLS analysis

```{r}
# fit.ols <- lm(scale(posttest) ~ scale(pretest) + condition, data = vaccW)

fit.ols <- lm(scale(posttest) ~ scale(pretest) * condition, contrasts = list(condition = contr.sum), data = vaccW)

summary(fit.ols)
```

## Beta Regression

Beta regression is a more appropriate approach to analyzing bounded, skewed, and heteroscedastic data. Which is exactly what our vaccine attitude scale measures are. 

For more info, see [this paper](https://www.ncbi.nlm.nih.gov/pubmed/16594767), cited below.

> Smithson M, Verkuilen J: A better lemon squeezer? Maximum-likelihood regression with beta-distributed dependent variables. Psychol Methods. 2006, 11 (1): 54-71.)

Also good to read the [betareg vignette](https://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf) (or run `vignette("betareg")`).

Now we can run a beta regression using the `betareg` package. This uses maximum-likelihood estimation and provides frequentist p-values, etc. Before conducting beta regression, the response variable must be rescaled on to the bounded (exclusive) interval $(0, 1)$.

```{r}

set.seed(123)

rescale_beta <- function(x, lower, upper) {
  # rescales onto the open interval (0,1)
  # rescales over theoretical bounds of measurement, specified by "upper" and "lower"
  # based on Smithson & Verkuilen (2006), though this is not as principled as you might think
  # see http://dx.doi.org/10.1037/1082-989X.11.1.54.supp

  N <- length(x)
  res <- (x - lower) / (upper - lower)
  res <- (res * (N - 1) + .5) / N

  return(as.vector(res))
}

fit.beta <- betareg(
  posttest ~ scale(pretest) + condition,
  data = vaccW %>% mutate(posttest = rescale_beta(posttest, 1, 6))
)

summary(fit.beta)
```

Comparing models ...

```{r}
fit.olsScaled <- lm(
  posttest ~ scale(pretest) * condition,
  data = vaccW %>% mutate(posttest = rescale_beta(posttest, 1, 6))
)

AIC(fit.olsScaled)
AIC(fit.beta)

```

The beta regression model is **MUCH** preferred by AIC. Its results are also more sensible, capturing the main effect of Disease Risk intervention without the spurious interaction induced by the scale bounds and/or regression to the mean. (Comparing models within the beta family, a non-interactive model is slightly preferred, which is what I showed above).


```{r}

vaccW %>%
  select(condition, pretest, posttest) %>%
  mutate(posttest = rescale_beta(posttest, 1, 6)) %>%
  bind_cols(predict(fit.beta) %>% as_tibble()) %>%
  rename(predictionBeta = value) %>%
  bind_cols(predict(fit.olsScaled) %>% as_tibble()) %>%
  rename(predictionOLS = value) %>%
  
  ggplot(aes(x = pretest, y = posttest, color = condition)) +
  geom_jitter(height = .1, width = .05, alpha = .8, shape = 1) +
  geom_line(aes(y = predictionBeta, linetype = "Beta")) +
  geom_line(aes(y = predictionOLS, linetype = "OLS")) +
  # geom_line(aes(y = predict(fit.ols, vaccW),
  #               colour = "OLS", linetype = "OLS")) +
  # scale_colour_manual("", values = c("red", "blue")) +
  scale_linetype_manual("", values = c("solid", "dashed")) +
  theme_bw()
```

## with BRMS

Now for fun, we can do this in BRMS for both models. Short answer is everything looks exactly the same.

```{r}
library(brms)
fit_brm_gauss <- brm(
  rescale_beta(posttest, 1, 6) ~ pretest + condition,
  data = vaccW,
  family = gaussian(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .80),
  cores = parallel::detectCores(),
  iter = 2000
)

```

```{r}
summary(fit_brm_gauss)
```

```{r}
fit_brm_beta <- brm(
  rescale_beta(posttest, 1, 6) ~ pretest + condition,
  data = vaccW,
  family = Beta(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)

```

```{r}
summary(fit_brm_beta)
```

We can look at posterior predictive checks to assess the appropriateness of the models.

```{r}
pp_check(fit_brm_gauss)
```

```{r}
pp_check(fit_brm_beta)
```

And we can compare the models using loo (analogous to AIC). Again, beta does far better.

```{r}
loo(fit_brm_gauss)
loo(fit_brm_beta)
```

Unfortunately, neither look perfect. The Gaussian model has the obvious problem of predicting out-of-range values. And they both fail to capture the "double-hump" of the distribution--it looks like there may be an over-representation of the second-strongest statement, probably a responding bias or tendency to temper agreement and avoid extremes?

In any case, we can create marginal effects plots from both models. I've rescaled posttest for both to permit the LOO comparison. They look pretty similar, but slightly smaller effects under beta, and of course it fit better.

```{r}
me1 <- marginal_effects(fit_brm_beta, effects = "condition", probs = c(.25, .75))
plt.me1 <- plot(me1,
  effects = "condition",
  plot = FALSE,
  rug = TRUE,
  theme = ggplot2::theme_get()
)[[1]] + labs(title = "Beta") + theme_bw()

me2 <- marginal_effects(fit_brm_gauss, effects = "condition", probs = c(.25, .75))
plt.me2 <- plot(me2,
  effects = "condition",
  plot = FALSE,
  rug = TRUE,
  theme = ggplot2::theme_get()
)[[1]] + labs(title = "Gaussian") + theme_bw()

library(gridExtra)
grid.arrange(plt.me1, plt.me2, ncol = 2)
```

## Predicting posttest from pretest, ordinal HLM

```{r}
vacc <- vaccW %>%
  rename(
    Healthy_post = Healthy_VaxscalePosttest,
    Diseases_post = Diseases_VaxScalePosttest_Reversed,
    Doctors_post = Doctors_VaxScalePostTest,
    SideEffects_post = Sideeffects_VaxScalePostTest_Reversed,
    PlanTo_post = Planto_VaxScalePostTest,
    Healthy_pre = Healthy_VaxscalePretest,
    Diseases_pre = Diseases_VaxScalePretest_Reversed,
    Doctors_pre = Doctors_VaxScalePreTest,
    SideEffects_pre = Sideeffects_VaccScalePreTest_Reversed,
    PlanTo_pre = Planto_VaxScalePreTest
  ) %>%
  # select(condition,ends_with("_pre"),ends_with("_post")) %>%
  gather(
    item,
    agree,
    ends_with("_pre"), 
    ends_with("_post")
  ) %>%
  mutate(phase = ifelse(grepl("_pre", item), "pretest", "posttest")) %>%
  mutate(item = sub("_pre", "", item)) %>%
  mutate(item = sub("_post", "", item))
  
```

So finally we can try a model that respects the ordinal nature of the original data. This uses the [proportional-odds cumulative logit model](https://onlinecourses.science.psu.edu/stat504/node/176). 

```{r echo=TRUE, results='hide'}
# untested: 3/19/18, 8:10 PM
fit_brm_cum_posttest <- brm(
  agree ~ pretest + condition + (1 | Participant) + (1 | item),
  data = vacc %>%
    filter(phase == "posttest"),
  family = cumulative(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)

```

```{r}
summary(fit_brm_cum_posttest)
```

We can't compare AIC because the underlying data are different here (different represenation), but that posterior predictive check looks essentially perfect.

```{r}
pp_check(fit_brm_cum_posttest)
```

# Predicting "response"

Another way of representing/modeling these data are to predict responses at both pre and posttest, with a model like the following:

```
response ~ timepoint * condition + (1|participant)
```

First, we can do so (incorrectly) assuming gaussian errors.

```{r}
fit_brm_gauss_resp <- brm(
  response ~ phase * condition + (1|Participant),
  data = vaccW %>%
    mutate(
      posttest = rescale_beta(posttest, 1, 6),
      pretest = rescale_beta(pretest, 1, 6)
    ) %>%
    gather(phase, response, pretest, posttest) %>%
    mutate(phase= ifelse(phase=="pretest",0,1)),
  family = gaussian(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)
```

```{r}
summary(fit_brm_gauss_resp)
```

Beta regression

```{r}
library(brms)

fit_brm_beta_resp <- brm(
  response ~ phase * condition + (1|Participant),
  data = vaccW %>%
    mutate(
      posttest = rescale_beta(posttest, 1, 6),
      pretest = rescale_beta(pretest, 1, 6)
    ) %>%
    gather(phase, response, pretest, posttest) %>%
    mutate(phase= ifelse(phase=="pretest",0,1)),
  family = Beta(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)

```

```{r}
summary(fit_brm_beta_resp)
```

```{r}
pp_check(fit_brm_gauss_resp)
pp_check(fit_brm_beta_resp)
```

```{r}
loo(fit_brm_gauss_resp)
loo(fit_brm_beta_resp)
```

Again, beta is preferred by LOO.


### predicing "response" from phase and condition, ordinal HLM

```{r echo=TRUE, results='hide'}
# untested: 3/19/18, 8:10 PM
fit_brm_cum_resp <- brm(
  agree ~ phase * condition + (1|Participant) + (1|item),
  data = vacc %>% mutate(phase = ifelse(phase=="pretest",0,1)),
  family = cumulative(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)
```

```{r}
summary(fit_brm_cum_resp)
```

And as in the previous set of results, posterior predictive checks look best for the ordinal regression model.

```{r}
pp_check(fit_brm_cum_resp)
```



# SessionInfo

```{r}
sessionInfo()
```