---
title: "PNAS re-analysis notebook"
author: "Derek Powell"
output: 
  html_notebook: 
    code_folding: hide
---

This is a notebook to re-analyze the data from the _PNAS_ paper "Countering Antivaccination Attitudes" using more appropriate statistical methods, such as ordinal HLM and beta regression. These analyses are being conducted in anticipation of a follow-up replication study.

These re-analyses are primarily concerned with identifying the best method for analyzing the data from the _PNAS_ study.

```{r, echo=FALSE, results='hide'}
library(tidyverse)
library(brms)
if (!require(betareg)) {
  install.packages("betareg")
}
library(betareg)
```

```{r}
vaccAll <- read.csv("vacc-hphh-pubdata.csv") %>% 
  as_tibble() %>% 
  mutate(Participant = seq(1:n())) %>%
  filter(EligibleToReturn==1)

vaccW <- vaccAll %>%
  mutate(pretest = PreTestVaccinationAttitude, posttest = PostTest.Vaccination.Attitude) %>%
  filter(Returned == 1, Excluded == 0) %>%
  mutate(Condition = relevel(Condition, ref="Control")) %>%
  rename(condition = Condition)

```


# Original Analysis (change scores)

First, we can approximately replicate the original analyses that were conducted. Original analyses looked at "change scores", computed as posttest score minus pretest score. The distribution of change scores is only moderately skewed, although it has heavy tails.

```{r}
vaccW <- vaccW %>% mutate(change = posttest - pretest)

vaccW %>%
  ggplot(aes(x=change)) + 
  geom_histogram(binwidth=.25)
```

```{r}

fit.change <- lm(change ~ condition, data = vaccW)
summary(fit.change)
```

The change score analysis has a few issues:

1. Fails to control for or reveal pretest differences between conditions
2. Fails to respect boundaries of original scale (e.g., for Ps near ceiling, negative change scores are largely uninformative)
3. Change scores influenced by both pre-test and posttest measurement error (likely contaminated by regression to the mean)
4. Assumes normal distribution (change scores somewhat non-normal)

Other approaches can avoid some or all of these issues. 

# Predicting posttest scores

## Normal regression

Unlike with change scores, posttest scores are not even approximately normal.

```{r}
vaccW %>%
  ggplot(aes(x=posttest)) + 
  geom_histogram(binwidth=.25)
```

```{r}
fit.ols <- lm(scale(posttest) ~ scale(pretest) * condition, 
              contrasts = list(condition = contr.treatment), 
              data = vaccW)

summary(fit.ols)
```

## Beta Regression

Beta regression is a more appropriate approach to analyzing bounded, skewed, and heteroscedastic data. Which is exactly what our vaccine attitude scale measures are. 

For more info, see [this paper](https://www.ncbi.nlm.nih.gov/pubmed/16594767), cited below.

> Smithson M, Verkuilen J: A better lemon squeezer? Maximum-likelihood regression with beta-distributed dependent variables. Psychol Methods. 2006, 11 (1): 54-71.)

Also good to read the [betareg vignette](https://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf) (or run `vignette("betareg")`).

Now we can run a beta regression using the `betareg` package. This uses maximum-likelihood estimation and provides frequentist p-values, etc. Before conducting beta regression, the response variable must be rescaled on to the bounded (exclusive) interval $(0, 1)$.

```{r}

set.seed(123)

rescale_beta <- function(x, lower, upper) {
  # rescales onto the open interval (0,1)
  # rescales over theoretical bounds of measurement, specified by "upper" and "lower"
  # based on Smithson & Verkuilen (2006), though this is not as principled as you might think
  # see http://dx.doi.org/10.1037/1082-989X.11.1.54.supp

  N <- length(x)
  res <- (x - lower) / (upper - lower)
  res <- (res * (N - 1) + .5) / N

  return(as.vector(res))
}

fit.beta <- betareg(
  posttest ~ scale(pretest) + condition,
  data = vaccW %>% mutate(posttest = rescale_beta(posttest, 1, 6))
)

summary(fit.beta)
```

Comparing models ...

```{r}
fit.olsScaled <- lm(
  posttest ~ scale(pretest) * condition,
  data = vaccW %>% mutate(posttest = rescale_beta(posttest, 1, 6))
)

AIC(fit.olsScaled)
AIC(fit.beta)

```

The beta regression model is **MUCH** preferred by AIC. Its results are also more sensible, capturing the main effect of Disease Risk intervention without the spurious interaction induced by the scale bounds and/or regression to the mean. (Comparing models within the beta family, a non-interactive model is slightly preferred, which is what I showed above).

We can plot the models ...

```{r}

vaccW %>%
  select(condition, pretest, posttest) %>%
  mutate(posttest = rescale_beta(posttest, 1, 6)) %>%
  bind_cols(predict(fit.beta) %>% as_tibble()) %>%
  rename(predictionBeta = value) %>%
  bind_cols(predict(fit.olsScaled) %>% as_tibble()) %>%
  rename(predictionOLS = value) %>%
  
  ggplot(aes(x = pretest, y = posttest, color = condition)) +
  geom_jitter(height = .1, width = .05, alpha = .8, shape = 1) +
  geom_line(aes(y = predictionBeta, linetype = "Beta")) +
  geom_line(aes(y = predictionOLS, linetype = "OLS")) +
  # geom_line(aes(y = predict(fit.ols, vaccW),
  #               colour = "OLS", linetype = "OLS")) +
  # scale_colour_manual("", values = c("red", "blue")) +
  scale_linetype_manual("", values = c("solid", "dashed")) +
  theme_bw()
```

### with BRMS

Now we'll switch to performing these analyses in BRMS, which will make subsequent analyses easier. The Bayesan appraoch replicates the frequentist results closely.

```{r}
library(brms)
fit_brm_gauss <- brm(
  rescale_beta(posttest, 1, 6) ~ pretest + condition,
  data = vaccW,
  family = gaussian(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .80),
  cores = parallel::detectCores(),
  iter = 2000
)

```

```{r}
summary(fit_brm_gauss)
```

```{r}
fit_brm_beta <- brm(
  rescale_beta(posttest, 1, 6) ~ pretest + condition,
  data = vaccW,
  family = Beta(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)

```

```{r}
summary(fit_brm_beta)
```

And we can compare the models using loo (analogous to AIC). Again, beta does far better.

```{r}
loo(fit_brm_gauss)
loo(fit_brm_beta)
```

Using BRMS allows us to look at posterior predictive checks to assess the appropriateness of the models.

```{r}
pp_check(fit_brm_gauss)
```

```{r}
pp_check(fit_brm_beta)
```

Unfortunately, neither look perfect. The Gaussian model has the obvious problem of predicting out-of-range values (note the movement in x-axis scale). And they both fail to capture the "double-hump" of the distribution--it looks like there may be an over-representation of the second-strongest statement, probably a responding bias or tendency to temper agreement and avoid extremes?

In any case, we can create marginal effects plots from both models. I've rescaled posttest for both to permit the LOO comparison. They look pretty similar, but slightly smaller effects under beta, and of course it fit better.

```{r}
me1 <- marginal_effects(fit_brm_beta, effects = "condition", probs = c(.25, .75))
plt.me1 <- plot(me1,
  effects = "condition",
  plot = FALSE,
  rug = TRUE,
  theme = ggplot2::theme_get()
)[[1]] + labs(title = "Beta") + theme_bw()

me2 <- marginal_effects(fit_brm_gauss, effects = "condition", probs = c(.25, .75))
plt.me2 <- plot(me2,
  effects = "condition",
  plot = FALSE,
  rug = TRUE,
  theme = ggplot2::theme_get()
)[[1]] + labs(title = "Gaussian") + theme_bw()

library(gridExtra)
grid.arrange(plt.me1, plt.me2, ncol = 2)
```

## Ordinal HLM

```{r}
vacc <- vaccW %>%
  rename(
    Healthy_post = Healthy_VaxscalePosttest,
    Diseases_post = Diseases_VaxScalePosttest_Reversed,
    Doctors_post = Doctors_VaxScalePostTest,
    SideEffects_post = Sideeffects_VaxScalePostTest_Reversed,
    PlanTo_post = Planto_VaxScalePostTest,
    Healthy_pre = Healthy_VaxscalePretest,
    Diseases_pre = Diseases_VaxScalePretest_Reversed,
    Doctors_pre = Doctors_VaxScalePreTest,
    SideEffects_pre = Sideeffects_VaccScalePreTest_Reversed,
    PlanTo_pre = Planto_VaxScalePreTest
  ) %>%
  # select(condition,ends_with("_pre"),ends_with("_post")) %>%
  gather(
    item,
    agree,
    ends_with("_pre"), 
    ends_with("_post")
  ) %>%
  mutate(phase = ifelse(grepl("_pre", item), "pretest", "posttest")) %>%
  mutate(item = sub("_pre", "", item)) %>%
  mutate(item = sub("_post", "", item))
  
```

So finally we can try a model that respects the ordinal nature of the original data. We'll use a hierarchica lversion of the [proportional-odds cumulative logit model](https://onlinecourses.science.psu.edu/stat504/node/176). 

```{r echo=TRUE, results='hide'}
# untested: 3/19/18, 8:10 PM
fit_brm_cum_posttest <- brm(
  agree ~ pretest + condition + (1 | Participant) + (1 | item),
  data = vacc %>%
    filter(phase == "posttest"),
  family = cumulative(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)

```

```{r}
summary(fit_brm_cum_posttest)
```

We can't compare AIC to the previous models because the underlying data are different here (different represenation), but the posterior predictive check looks essentially perfect.

```{r}
pp_check(fit_brm_cum_posttest)
```

# Predicting "response" at pretest and posttest

Another way of representing/modeling these data are to predict responses at both pre and posttest, with a model like the following:

```
response ~ timepoint * condition + (1|participant)
```

## Normal regression

First, we can do so (incorrectly) assuming gaussian errors.

```{r}
fit_brm_gauss_resp <- brm(
  response ~ phase * condition + (1|Participant),
  data = vaccW %>%
    mutate(
      posttest = rescale_beta(posttest, 1, 6),
      pretest = rescale_beta(pretest, 1, 6)
    ) %>%
    gather(phase, response, pretest, posttest) %>%
    mutate(phase= ifelse(phase=="pretest",0,1)),
  family = gaussian(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)
```

```{r}
summary(fit_brm_gauss_resp)
```

## Beta regression

```{r}
library(brms)

fit_brm_beta_resp <- brm(
  response ~ phase * condition + (1|Participant),
  data = vaccW %>%
    mutate(
      posttest = rescale_beta(posttest, 1, 6),
      pretest = rescale_beta(pretest, 1, 6)
    ) %>%
    gather(phase, response, pretest, posttest) %>%
    mutate(phase= ifelse(phase=="pretest",0,1)),
  family = Beta(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)

```

```{r}
summary(fit_brm_beta_resp)
```

```{r}
pp_check(fit_brm_gauss_resp)
pp_check(fit_brm_beta_resp)
```

```{r}
loo(fit_brm_gauss_resp)
loo(fit_brm_beta_resp)
```

Again, beta is preferred by LOO.

## Ordinal HLM

```{r echo=TRUE, results='hide'}
# untested: 3/19/18, 8:10 PM
fit_brm_cum_resp <- brm(
  agree ~ phase * condition + (1|Participant) + (1|item),
  data = vacc %>% mutate(phase = ifelse(phase=="pretest",0,1)),
  family = cumulative(), # student(), #cumulative(), #bernoulli(), etc
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)
```

```{r}
summary(fit_brm_cum_resp)
```

And as in the previous set of results, posterior predictive checks look best for the ordinal regression model. Subjectively however, they look poorer than they did in the model predicting posttest.

```{r}
pp_check(fit_brm_cum_resp)
```

# Conclusions

Provisionally, I prefer to use the `posttest ~ pretest + condition` model form using the ordinal HLM across raw likert ratings. There are a few benefits to the model formula and to that data/distributional choice: (1) I think it's the easiest to interpret, (2) adding an interaction can model pretest attitudes moderating condition effects, (3) the model offered the best fit by posterior predictive checks, (4) models predicting posttest result in better model fit than models predicting "response", which should support "good behavior" generally from the model, and (5) it seems to be the most commmonly preferred approach among other researchers.

# SessionInfo

```{r}
sessionInfo()
```