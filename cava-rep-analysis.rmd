---
title: "Countering AntiVaccination Attitudes Replication"
author: "Derek Powell & Kara Weisman"
output: 
  html_notebook: 
    code_folding: hide
---

__NOTE: These are preliminary analyses as of 4/9/18, 10:55 AM__

This is an R notebook to analyze the data from the Countering AntiVaccination Attitudes (CAVA) replication study. This replication roject was [preregistered on OSF](https://osf.io/j5n4e/) and all data and code are available on [github](https://github.com/derekpowell/cava-rep).

- Part 1 of the study was conducted on March 21, 2018
- Part 2 of the study was conducted on March 22, 2018 (with a few Ps collected March 23, 2018)
- Part 3 of the study was conducted March 29, 2018 (with a few Ps collected March 30, 2018) 

```{r}
# load packages
library(tidyverse)
library(brms)
library(tidybayes)

if (!require(betareg)) {
  install.packages("betareg")
}
library(betareg)

if (!require(effsize)) {
  install.packages("effsize")
}
library(effsize)

# Define some functions ...
read_qualtrics_csv <- function(fname) {
  df <- read.csv(fname, skip = 3, header = F)
  headers <- as.matrix(read.csv(fname, skip = 0, header = F, nrows = 1, as.is = T))
  colnames(df) <- headers
  df <- df[which(df[, "DistributionChannel", ] == "anonymous"), ] # remove survey previews
  total_n <- nrow(df)

  # remove unused qualtrics variables
  remove_cols <- c(
    "RecipientLastdata",
    "RecipientFirstdata",
    "RecipientEmail",
    "Finished",
    "ResponseId",
    "ExternalReference",
    "DistributionChannel",
    "UserLanguage",
    "Status"
  )

  df <- df[, -which(colnames(df) %in% remove_cols)]

  return(df)
}

rescale_beta <- function(x, lower, upper) {
  # rescales onto the open interval (0,1)
  # rescales over theoretical bounds of measurement, specified by "upper" and "lower"
  # based on Smithson & Verkuilen (2006), though this is not as principled as you might think
  # see http://dx.doi.org/10.1037/1082-989X.11.1.54.supp

  N <- length(x)
  res <- (((x - lower) / (upper - lower)) * (N - 1) + .5) / N

  return(as.vector(res))
}

inv_rescale_beta <- function(scaled_x, lower, upper) {
  # inverts rescaling onto the open interval (0,1)
  # assuming original rescaling over theoretical bounds of measurement, specified by "upper" and "lower"
  # based on Smithson & Verkuilen (2006), though this is not as principled as you might think
  # see http://dx.doi.org/10.1037/1082-989X.11.1.54.supp
  
  N <- length(scaled_x)

  res <- (scaled_x * N - 1/2)/(N-1)
  res <- res * (upper - lower) + lower
  return(as.vector(res))
}

gg_marginal_effects <- function(model, effects, probs=c(0.25, 0.75)) {
  me <- marginal_effects(model, effects=effects, probs=c(.25,.75))
  plt.me <- plot(me, 
                  effects=effects,
                  plot=FALSE,
                  rug=TRUE,
                  theme=ggplot2::theme_get()
                  )[[1]] +
    theme_minimal()
} 

devtools::source_gist(id = "f1994c0f8325abbc5d300600744af39d", filename="cbrm.R")

```

```{r}
# working from raw qualtrics with workerIds anonymized

# Data preprocessing ...
d_pre <- read_qualtrics_csv("data/Vaccine+PNAS+replication+day1-2018-03-23.csv") %>%
  select(workerId, age:Vax1_Vax1_36, eligible, study_time, payment)

d_post <- read_qualtrics_csv("data/Vaccine+PNAS+replication+day2-2018-04-02.csv") %>%
  select(workerId, condition, vax2_Vax1_1:vax2_Vax1_36, parent:SC1)

d_merge <- merge(d_pre,d_post, by="workerId") %>%
  as_tibble() %>%
  filter(SC1==4) %>% # passed all checks
  filter(Attention==1) # paid attention

# rename variables and do reverse coding

d_untidy <- d_merge %>%
  select(-Vax1_Vax1_check,-vax2_Vax1_check) %>%
  rename(
    vax_pre_risks = Vax1_Vax1_1,
    vax_pre_herd = Vax1_Vax1_2,
    vax_pre_plan = Vax1_Vax1_3,
    vax_pre_uncommon = Vax1_Vax1_4,
    vax_pre_doctors = Vax1_Vax1_5,
    vax_pre_autism = Vax1_Vax1_6,
    vaxIntent_pre_Iwould = Vax1_Vax1_33,
    vaxIntent_pre_spreadOut = Vax1_Vax1_34,
    vaxIntent_pre_confident = Vax1_Vax1_35,
    vaxIntent_pre_wouldNever = Vax1_Vax1_36,
    vax_post_risks = vax2_Vax1_1,
    vax_post_herd = vax2_Vax1_2,
    vax_post_plan = vax2_Vax1_3,
    vax_post_uncommon = vax2_Vax1_4,
    vax_post_doctors = vax2_Vax1_5,
    vax_post_autism = vax2_Vax1_6,
    vaxIntent_post_Iwould = vax2_Vax1_33,
    vaxIntent_post_spreadOut = vax2_Vax1_34,
    vaxIntent_post_confident = vax2_Vax1_35,
    vaxIntent_post_wouldNever = vax2_Vax1_36
  ) %>%
  mutate( # implement reverse coding
    vax_pre_risks = 7 - vax_pre_risks,
    vax_post_risks = 7 - vax_post_risks,
    vax_pre_uncommon = 7 - vax_pre_uncommon,
    vax_post_uncommon = 7 - vax_post_uncommon,
    vaxIntent_pre_spreadOut = 7 - vaxIntent_pre_spreadOut,
    vaxIntent_post_spreadOut = 7 - vaxIntent_post_spreadOut,
    vaxIntent_pre_wouldNever = 7 - vaxIntent_pre_wouldNever,
    vaxIntent_post_wouldNever = 7 - vaxIntent_post_wouldNever
  ) %>% 
  mutate( # add duplicates to cover both scales
    vaxIntent_pre_risks = vax_pre_risks,
    vaxIntent_post_risks = vax_post_risks
  ) 

# more munging to tidy data ...
d <- d_untidy %>%
  gather(item, rating, contains("vax")) %>%
  mutate(
    rating = as.integer(rating),
    phase = ifelse(grepl("_pre_",item),"pretest","posttest"),
    Scale = ifelse(grepl("vaxIntent_",item), "vaxIntent", "vaxAttitude"),
    condition = factor(condition, 
                       levels=c("bird","autism","cdc_danger"),
                       labels=c("Control","Autism Correction", "Disease Risk"))
    ) %>%
  mutate(condition = relevel(condition, ref="Control")) %>%
  mutate(Scale = ifelse(grepl("autism",item), "autism", Scale)) %>%
  mutate(item=gsub("vax_pre_","", item)) %>%
  mutate(item=gsub("vax_post_","", item)) %>%
  mutate(item=gsub("vaxIntent_pre_","", item)) %>%
  mutate(item=gsub("vaxIntent_post_","", item)) %>%
  mutate(phase = factor(phase, levels = c("pretest","posttest","posttest2")))

# and reducing to a "summary" dataset
d_sum <- d %>%
  filter(Scale=="vaxAttitude") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating) %>%
  mutate(change = posttest - pretest)
```


# Preliminaries {.tabset}

## Check scale reliability and confirm proper reverse coding

For vaccine attitudes ...

```{r}
d %>% 
  filter(Scale == "vaxAttitude", phase == "pretest") %>% 
  spread(item, rating) %>% 
  select(doctors:uncommon) %>% 
  psych::alpha()

d %>% 
  filter(Scale == "vaxAttitude", phase == "posttest") %>% 
  spread(item, rating) %>% 
  select(doctors:uncommon) %>% 
  psych::alpha()
```

for vaccine intentions ...

```{r}
d %>%
  filter(Scale == "vaxIntent", phase == "pretest") %>%
  spread(item, rating) %>%
  select(confident:wouldNever) %>%
  psych::alpha()

d %>%
  filter(Scale == "vaxIntent", phase == "posttest") %>%
  spread(item, rating) %>%
  select(confident:wouldNever) %>%
  psych::alpha()
```

Scales have been properly coded and can be considered reliable. 

## Testing for pretest differences

Before considering change scores, we should confirm there are no meaningful pretest differences between conditions. The plot below suggests 

```{r, fig.align="center"}
# d_sum %>%
#   ggplot(aes(x=pretest, fill=condition)) +
#   geom_density(alpha=.25) +
#   theme_minimal()

d_sum %>%
  ggplot(aes(x = condition, fill = condition, colour = condition, y = pretest)) +
  geom_violin(width = .5, alpha = .25) +
  stat_summary(fun.data = "mean_cl_boot", fun.args = (list(B = 10000, conf.int = .95)), color = "grey20", shape = 16) +
  theme_minimal() +
  labs(title = "Pretest Scores", y = "Pretest Scores", x = "Condition") +
  annotate("text", label = "Error bars represent 95% bootstrap CI of the mean", x = 2, y = .5)
```

```{r}
fit_pre <- betareg(rescale_beta(pretest,1,6) ~ condition, data=d_sum)
summary(fit_pre)
```

Participants in the autism correction and disease risk conditions had slightly (tho non-significantly) higher pretest scores than participants in the control condition. Presumably, this should have made it all the more difficult to improve attitudes in those conditions relative to control.

# I. Direct Replication

## Plotting Change Scores

Plotting to replicate Figure 1 from _PNAS_ paper.

```{r}
# standard error function
stderr <- function(x) {
          sqrt(var(x[!is.na(x)]) / length(x[!is.na(x)]))
}

d %>%
  filter(Scale=="vaxAttitude") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating) %>%
  summarize(change_score = posttest-pretest) %>%
  group_by(condition) %>%
  summarize(mean_change = mean(change_score),
            ul = mean(change_score) + stderr(change_score),
            ll = mean(change_score) - stderr(change_score)) %>%
  ggplot(aes(x=condition, fill=condition, y = mean_change, ymin=ll, ymax=ul)) +
  geom_bar(stat="identity") +
  geom_errorbar(width=.25) +
  theme_minimal() +
  labs(title="Change Score Analysis",
       subtitle = "Error bars are standard errors",
       y = "Mean Change Score",
       x = "Condition",
       fill = "Condition")
```

Anaysis of change scores shows significant improvement in disease risk condition. This analysis replicates the original analyses and the primary pre-registered analysis. __We directly replicate the original findings.__

```{r}
fit_aov <- aov(change ~ condition, data = d_sum)
summary(fit_aov)

t.test(change ~ condition, data = d_sum %>% filter(condition!="Autism Correction"))
dr_control_D <- suppressWarnings(cohen.d(change ~ condition, data = d_sum %>% filter(condition!="Autism Correction"))$estimate)

t.test(change ~ condition, data = d_sum %>% filter(condition!="Control"))
dr_autism_D <- suppressWarnings(cohen.d(change ~ condition, data = d_sum %>% filter(condition!="Control"))$estimate)
```

Comparing the magnitude of the effects, the effect sizes in this replication study are smaller than what was originally observed, with the Disease Risk vs Control D = `r as.numeric(dr_control_D) %>% round(3)` (compared with .41 originally) and Disease Risk vs Autism Correction D = `r as.numeric(dr_autism_D) %>% round(3)` (compared with .33 originally).

# II. "Best Model" Replication

ANOVA on change scores is not the optimal approach to analyzing these data. Next, we explore some better ways of modeling the data from this experiment, and evaluate whether these improved statistical models also support the conclusions drawn in the original paper.

The change score analysis has a few issues:

1. Fails to control for or reveal pretest differences between conditions
2. Change scores influenced by both pre-test and posttest measurement error (likely contaminated by regression to the mean)
3. Fails to respect boundaries of original scale (e.g., for Ps near ceiling, negative change scores are largely uninformative)
4. Assumes normal distribution (change scores somewhat non-normal)

Other approaches can avoid some or all of these issues. 

## Posttest scores by condition, controlling for pretest

### Linear regression model

Using a linear regression predicting posttest from condition and pretest begins to mitigate the first two issues: it helps to control for pretest differences among conditions (though these can still complicate inference) and allows for modeling error in predicting posttest scores from pretest scores.

```{r}
fit_lm <- lm(posttest ~ scale(pretest) + condition,
           data=d_sum)

summary(fit_lm)
```

However, this analyses fails to account for the boundedness of the scale nor the non-normal distribution of responses.

### Beta Regression

Beta regression is a more appropriate approach to analyzing bounded, skewed, and heteroscedastic data--three properties of the vaccine attitudes scale responses. For more info, see [this paper](https://www.ncbi.nlm.nih.gov/pubmed/16594767): Smithson M, Verkuilen J (2006) A better lemon squeezer? Maximum-likelihood regression with beta-distributed dependent variables. _Psychol Methods._ 11 (1): 54-71.

The use of Beta regression addresses all four of the major concerns in the change score analyses.

```{r}
fit_beta <- betareg(rescale_beta(posttest,1,6) ~ scale(pretest) * condition,
           data=d_sum)

summary(fit_beta)
```

### Bayesian beta regression

We also conduct beta regression using bayesian estimation in BRMS.

```{r}
fit_brm_beta <- cbrm(
  rescale_beta(posttest, 1, 6) ~ scale(pretest) + condition,
  data = d_sum,
  family = Beta(),
  sample_prior = TRUE,
  control = list(adapt_delta = .99),
  # cores = parallel::detectCores(),
  iter = 3000,
  cached_file = "fit_brm_beta.rds"
)
```

This allows us to plot condition effects estimated from the model while marignalizing over pretest scores, along with 50% credible intervals.

```{r}
summary(fit_brm_beta)
gg_marginal_effects(fit_brm_beta, "condition") %>% plot()
```

### Hierarchical ordinal regression

A final concern in all of the above analyses, on change scores or on posttest scores, is the use of "scores". That is, the averaging of several ordinal likert scale ratings into a continuous scale variable. Using hierarchical ordinal regression, we can avoid this averaging, and directly model the observed data. This accounts for the ordinal nature of the underlying response variable, as well as possible differences among scale items that are lost to averaging.

```{r}
fit_brm_ord <- cbrm(
  posttest ~ pretest + condition + (1 | workerId) + (1 | item),
  data = d %>% filter(Scale=="vaxAttitude") %>% spread(phase, rating),
  family = cumulative(), 
  cached_file = "fit_brm_ord.rds",
  sample_prior = TRUE,
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)
```

```{r}
summary(fit_brm_ord)
gg_marginal_effects(fit_brm_ord, "condition") %>% plot()
```

## Delayed post-test results

In addition to replicating the original study, we also 

```{r}
d_post2 <- read_qualtrics_csv("data/Vaccine+PNAS+replication+day3-2018-04-02.csv") %>%
  filter(SC1 == 1) %>%
  select(workerId, contains("Vax3")) %>%
  rename(
    vax_post2_risks = Vax3_Vax1_1,
    vax_post2_herd = Vax3_Vax1_2,
    vax_post2_plan = Vax3_Vax1_3,
    vax_post2_uncommon = Vax3_Vax1_4,
    vax_post2_doctors = Vax3_Vax1_5,
    vax_post2_autism = Vax3_Vax1_6,
    vaxIntent_post2_Iwould = Vax3_Vax1_33,
    vaxIntent_post2_spreadOut = Vax3_Vax1_34,
    vaxIntent_post2_confident = Vax3_Vax1_35,
    vaxIntent_post2_wouldNever = Vax3_Vax1_36
  ) %>%
  mutate(
    vax_post2_risks = 7 - vax_post2_risks,
    vax_post2_uncommon = 7 - vax_post2_uncommon,
    vaxIntent_post2_spreadOut = 7 - vaxIntent_post2_spreadOut,
    vaxIntent_post2_wouldNever = 7 - vaxIntent_post2_wouldNever
  ) %>%
  mutate(
    vaxIntent_post2_risks = vax_post2_risks
  )

d_untidy2 <- d_post2 %>% 
  # filter(Vax3_Vax1_check == 5) %>%
  # select(-Vax3_Vax1_check) %>%
  merge(d_untidy, by="workerId") %>%
  select(-Vax3_Vax1_check) %>%
  gather(item, rating, contains("vax"))

d2 <- d_untidy2 %>%
  mutate(
    rating = as.integer(rating),
    phase = ifelse(grepl("_pre_", item),"pretest",
                   ifelse(grepl("_post2_", item), "posttest2", "posttest")),
    Scale = ifelse(grepl("vaxIntent_",item), "vaxIntent", "vaxAttitude"),
    condition = factor(condition, 
                       levels=c("bird","autism","cdc_danger"),
                       labels=c("Control","Autism Correction", "Disease Risk"))
    ) %>%
  mutate(condition = relevel(condition, ref="Control")) %>%
  mutate(Scale = ifelse(grepl("autism",item), "autism", Scale)) %>%
  mutate(item=gsub("vax_pre_","", item)) %>%
  mutate(item=gsub("vax_post_","", item)) %>%
  mutate(item=gsub("vaxIntent_pre_","", item)) %>%
  mutate(item=gsub("vaxIntent_post_","", item)) %>%
  mutate(item=gsub("vax_post2_","", item)) %>%
  mutate(item=gsub("vaxIntent_post2_","", item)) %>%
  group_by(workerId, condition, phase, Scale, item) %>% # somehow got a duplicate row
  filter(row_number(rating) == 1) %>% # so remove it
  ungroup()

# and reducing to a "summary" dataset
# d_sum2 <- d2 %>%
#   filter(Scale=="vaxAttitude") %>%
#   group_by(workerId, condition, phase) %>%
#   summarize(mean_rating = mean(rating)) %>%
#   spread(phase, mean_rating) %>%
#   mutate(change = posttest - pretest)
  
```

### Plotting Raw data

Plotting change scores relative to pretest at posttest and delayed posttest, it looks like disease risk is effective immediately, but not effect compared to control at delay.

```{r}
d2 %>%
  filter(Scale == "vaxAttitude") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating) %>%
  mutate(
    change = posttest - pretest,
    change2 = posttest2 - pretest
  ) %>%
  gather(change_phase, Score, change, change2) %>%
  group_by(condition, change_phase) %>%
  summarize(
    mean_score = mean(Score),
    ul = mean(Score) + stderr(Score),
    ll = mean(Score) - stderr(Score)
  ) %>%
  ggplot(aes(x = change_phase, y = mean_score, color = condition, ymin = ll, ymax = ul)) +
  geom_point(stat = "identity", position = position_dodge(width = .5)) +
  geom_errorbar(position = position_dodge(width = .5), width = .25)
```

However change scores have issues. Let's just look at the raw overall means across each phase to get a sense of things ...

```{r}
d2 %>%
    filter(Scale=="vaxAttitude") %>%
    group_by(workerId, condition, phase) %>%
    summarize(mean_rating = mean(rating)) %>%
    group_by(condition, phase) %>%
    summarize(mean_resp = mean(mean_rating),
               ul = mean(mean_rating) + stderr(mean_rating),
              ll = mean(mean_rating) - stderr(mean_rating)) %>%
  mutate(phase = factor(phase, levels = c("pretest","posttest","posttest2"))) %>%
  ggplot(aes(x=phase, y = mean_resp, color = condition, ymin = ll, ymax = ul)) +
  geom_point(stat="identity",position=position_dodge(width=.5)) +
  geom_errorbar(position=position_dodge(width=.5), width=.25)

```

### Modeling

First, let's model things using a beta regression. First, let's ask if there's a significant condition difference at delayed postest among the conditions, controlling for original pretest scores.

From this, it looks like there is not. There's also no magical delayed backfire effect of autism correction.

```{r}
fit_beta2 <- betareg(rescale_beta(posttest2, 1, 6) ~ scale(pretest) + condition,
                     data = d2 %>% 
                       filter(Scale=="vaxAttitude") %>%
                       group_by(workerId, condition, phase) %>%
                       summarize(mean_rating = mean(rating)) %>%
                       spread(phase, mean_rating)
                     )

summary(fit_beta2)
```

Now let's try to put everything together into one model. This model will predict both immediate and delayed posttest responses from pretest and condition, along with a "time" variable coding whether they are immediate or delayed responses. I'll let this interact with condition, since the change scores suggest the effects of time might be different in the different conditions.

```{r}
d2_sum <- d2 %>%
  filter(Scale == "vaxAttitude") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating) %>%
  gather(time, response, posttest, posttest2)

# fit_beta2 <- brm(rescale_beta(response,1,6) ~ scale(pretest) + time * condition + (1|workerId), data = d2_sum )

fit_brm_beta2 <- cbrm(
  rescale_beta(response,1,6) ~ scale(pretest) + time * condition + (1|workerId),
  data = d2_sum,
  family = Beta(), # student(), #cumulative(), #bernoulli(), etc
  cached_file = "fit_brm_beta2.rds",
  control = list(adapt_delta = .95),
  sample_prior = TRUE,
  cores = parallel::detectCores(),
  iter = 2000
)

```

```{r}
summary(fit_brm_beta2)
gg_marginal_effects(fit_brm_beta2, "time:condition") %>% plot()
```


Using the marignal effects plot lets us pull out the model predictions without having to worry about interpreting beta regression coefficients. Here I'm using 50% credible intervals. It looks like, at immediate test, disease risk shows a significant improvement over control and autism correction, as previously observed. At delayed test, it's fallen back down but it looks like it's still expected to be a bit better than control. I'll have to test that hypothesis inside the model somehow. Likewise, there's a hint that autism correction looks better than control in this sample/analysis, which I'll have to look at more closely.

It's actually not that crazy. Let's also rerun the model as a hierarchical ordinal regression ...

```{r}
fit_brm_ord2 <- cbrm(
  response ~ phase * condition + (1 | workerId) + (1|item),
  data = d2 %>%
    filter(Scale == "vaxAttitude") %>%
    group_by(workerId, condition, phase) %>%
    select(item, phase, condition, workerId, rating) %>%
    ungroup() %>%
    spread(phase, rating) %>%
    gather(phase, response, posttest, posttest2) %>%
    mutate(phase = relevel(factor(phase), ref = "posttest")),
  family = cumulative(),
  cached_file = "fit_brm_ord2.rds",
  sample_prior = TRUE,
  control = list(adapt_delta = .85),
  cores = parallel::detectCores(),
  iter = 2000
)
```

```{r}
summary(fit_brm_ord2)
hypothesis(fit_brm_ord2, "conditionDiseaseRisk + phaseposttest2:conditionDiseaseRisk > 0")
# maybe look directly at posterior distributions? chance to try tidybayes?
```

### Predicting response

Now that we have multiple posttest periods, it might be preferrable to use a model that predicts responses across all phases. This feels a bit less piecemeal than the past models, although it might be less powerful.

Model:

```
response ~ phase * condition + (1|participant)
```


```{r}
fit_brm_beta2_resp <- cbrm(
  rescale_beta(response, 1, 6) ~ phase * condition + (1 | workerId),
  data = d2 %>%
    filter(Scale == "vaxAttitude") %>%
    mutate(phase = relevel(factor(phase), ref = "pretest")) %>%
    group_by(workerId, condition, phase) %>%
    summarize(response = mean(rating)),
  family = Beta(),
  seed = 12345,
  cached_file = "fit_brm_beta2_resp.rds",
  sample_prior = TRUE,
  control = list(adapt_delta = .95),
  cores = parallel::detectCores(),
  iter = 2000
)
```

```{r}
summary(fit_brm_beta2_resp)
gg_marginal_effects(fit_brm_beta2_resp, "phase:condition") %>% plot()
```

That seems like a pretty clear summary of the results, in a model that captures the pretest differences explicitly.

Just to be safe, we can do it as a hierarchical ordinal model as well. 

```{r}
# fit_brm_ord2b <- brm(
#   rating ~ phase * condition + (1 | workerId) + (1 | item),
#   data = d2 %>%
#     filter(Scale == "vaxAttitude") %>%
#     mutate(phase = factor(phase, levels = c("pretest","posttest","posttest2"))) %>%
#     select(workerId, item, phase, condition, rating),
#   family = cumulative(),
#   sample_prior = TRUE,
#   control = list(adapt_delta = .85),
#   cores = parallel::detectCores(),
#   iter = 2000
# )
```

```{r}
# summary(fit_brm_ord2b)
# gg_marginal_effects(fit_brm_ord2b, "phase:condition") %>% plot()
```

So at the end of all that, it's not really all that clear what happened after a delay! The conclusions we'd draw seem to depend heavily on the analytic choices we make.

# III. "Vaccine intent" scale results

Here's a plot of the change scores for the vaccine intent scale across conditions. The differences here are somewhat smaller than for "vaccine attitudes", but they appear significant.

```{r}
d %>%
  filter(Scale=="vaxIntent") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating) %>%
  summarize(change_score = posttest-pretest) %>%
  group_by(condition) %>%
  summarize(mean_change = mean(change_score),
            ul = mean(change_score) + stderr(change_score),
            ll = mean(change_score) - stderr(change_score)) %>%
  ggplot(aes(x=condition, fill=condition, y = mean_change, ymin=ll, ymax=ul)) +
  geom_bar(stat="identity") +
  geom_errorbar(width=.25) +
  theme_minimal() +
  labs(title="Change Score Analysis: Vaccine Intent",
       subtitle = "Error bars are standard errors")
```

```{r}
d %>%
  filter(Scale=="vaxIntent") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating) %>%
  summarize(change_score = posttest-pretest) %>%
  ggplot(aes(x = change_score, fill = condition)) +
  # geom_histogram(binwidth = 0.25, alpha = 0.25, position = "identity") +
  geom_density(alpha = 0.25, position = "identity") +
  geom_vline(xintercept = 0, lty = 2) +
  theme_minimal() +
  labs(title = "Distribution of Change Scores: Vaccine Intent",
       x = "Change Score", y = "Density", fill = "Condition")

d %>%
  filter(Scale=="vaxIntent") %>%
  mutate(phase = factor(phase, levels = c("pretest", "posttest"))) %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  ggplot(aes(x = phase, color = condition, y = mean_rating)) +
  facet_grid(~ condition) +
  geom_line(aes(group = workerId), alpha = 0.1, size = 1) +
  geom_point(alpha = 0.1, size = 1) +
  scale_y_continuous(limits = c(0, 6), breaks = 0:6) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Changes at the Individual Level: Vaccine Intent",
       x = "Timepoint", y = "Vaccine Intent Score")
```

## Linear regression

Under the linear regression model, the contrast between control and disease risk conditions is just barely non-significant at the $\alpha = .05$ level.

```{r}
d_sum_intent <- d %>%
  filter(Scale == "vaxIntent") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating)

fit_lm_intent <- lm(posttest ~ scale(pretest) + condition, data = d_sum_intent)
summary(fit_lm_intent)
```

## Beta regression

However, by beta regression, the main contrast is significant.

```{r}
d_sum_intent <- d %>%
  filter(Scale == "vaxIntent") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating)

fit_beta_intent <- betareg(
  rescale_beta(posttest, 1, 6) ~ scale(pretest) * condition,
  data = d_sum_intent
)

summary(fit_beta_intent)
```

## Ordinal HLM

Again, this is likely the most appropriate analysis for these data.

```{r}
fit_brm_ord_intent <- cbrm(
  posttest ~ pretest + condition + (1 | workerId) + (1 | item),
  data = d %>% filter(Scale == "vaxIntent") %>% spread(phase, rating),
  family = cumulative(), # student(), #cumulative(), #bernoulli(), etc
  sample_prior = TRUE,
  cached_file = "fit_brm_ord_intent.rds",
  control = list(adapt_delta = .95),
  cores = parallel::detectCores(),
  iter = 2000
)
```

```{r}
summary(fit_brm_ord_intent)
# gg_marginal_effects(fit_brm_ord_intent, "condition") %>% plot()
```

## Delayed post-test and vaccine intentions

### Plotting Raw data

Plotting change scores relative to pretest at posttest and delayed posttest, it looks like disease risk is effective immediately, but not effect compared to control at delay. There's actually even some sign of backfire for autism correction at delay, unclear what that means.

```{r}
d2 %>%
  filter(Scale == "vaxIntent") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating) %>%
  mutate(
    change = posttest - pretest,
    change2 = posttest2 - pretest
  ) %>%
  gather(change_phase, Score, change, change2) %>%
  group_by(condition, change_phase) %>%
  summarize(
    mean_score = mean(Score),
    ul = mean(Score) + stderr(Score),
    ll = mean(Score) - stderr(Score)
  ) %>%
  ggplot(aes(x = change_phase, y = mean_score, color = condition, ymin = ll, ymax = ul)) +
  geom_point(stat = "identity", position = position_dodge(width = .5)) +
  geom_errorbar(position = position_dodge(width = .5), width = .25)
```

However change scores have issues. Let's just look at the raw overall means across each phase to get a sense of things ...

```{r}
d2 %>%
    filter(Scale=="vaxIntent") %>%
    group_by(workerId, condition, phase) %>%
    summarize(mean_rating = mean(rating)) %>%
    group_by(condition, phase) %>%
    summarize(mean_resp = mean(mean_rating),
               ul = mean(mean_rating) + stderr(mean_rating),
              ll = mean(mean_rating) - stderr(mean_rating)) %>%
  mutate(phase = factor(phase, levels = c("pretest","posttest","posttest2"))) %>%
  ggplot(aes(x=phase, y = mean_resp, color = condition, ymin = ll, ymax = ul)) +
  geom_point(stat="identity",position=position_dodge(width=.5)) +
  geom_errorbar(position=position_dodge(width=.5), width=.25)

```

Now it's pretty clear that the "backfire" of the autism correction looks like an artifact of the change scores, as it's higher than control in each time point.

### Modeling

First, let's model things using a beta regression. First, let's ask if there's a significant condition difference at delayed postest among the conditions, controlling for original pretest scores.

From this, it looks like it's close but not quite for disease risk.

```{r}
fit_beta_intent <- betareg(rescale_beta(posttest2, 1, 6) ~ scale(pretest) + condition,
                     data = d2 %>% 
                       filter(Scale=="vaxIntent") %>%
                       group_by(workerId, condition, phase) %>%
                       summarize(mean_rating = mean(rating)) %>%
                       spread(phase, mean_rating)
                     )

summary(fit_beta_intent)
```

```{r}
# fit_brm_beta_intent_delay <- cbrm(
#   rescale_beta(posttest2, 1, 6) ~ scale(pretest) + condition,
#   data = d2 %>% 
#     filter(Scale == "vaxIntent") %>%
#     group_by(workerId, condition, phase) %>%
#     summarize(mean_rating = mean(rating)) %>%
#     spread(phase, mean_rating),
#   family = Beta(),
#   prior = c(set_prior("normal(0,.5)", coef = "conditionDiseaseRisk"),
#             set_prior("normal(0,.5)", coef = "conditionAutismCorrection")),
#   cached_file = "fit_brm_beta_intent_delay.rds",
#   sample_prior = TRUE,
#   control = list(adapt_delta = .99),
#   cores = parallel::detectCores(),
#   iter = 3000,
#   warmup = 2000
# )
```


Now let's try to put everything together into one model. This model will predict both immediate and delayed posttest responses from pretest and condition, along with a "time" variable coding whether they are immediate or delayed responses. I'll let this interact with condition, since the change scores suggest the effects of time might be different in the different conditions.

```{r}
d2_sum <- d2 %>%
  filter(Scale == "vaxIntent") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating) %>%
  gather(time, response, posttest, posttest2)

fit_brm_beta_intent <- cbrm(
  rescale_beta(response,1,6) ~ scale(pretest) + time * condition + (1|workerId),
  data = d2_sum,
  family = Beta(), # student(), #cumulative(), #bernoulli(), etc
  prior = c(prior(normal(0,1), class = b), 
            prior(normal(0,5), coef=scalepretest)),
  cached_file = "fit_brm_beta_intent.rds",
  sample_prior = TRUE,
  control = list(adapt_delta = .95),
  cores = parallel::detectCores(),
  iter = 2000
)

```

```{r}
summary(fit_brm_beta_intent)
gg_marginal_effects(fit_brm_beta_intent, "time:condition") %>% plot()

# test P that diseaseRisk differs from control at T2, after controlling for pretest scores
fit_brm_beta_intent %>%
  spread_samples(b_timeposttest2, 
                 b_conditionDiseaseRisk, 
                 `b_timeposttest2:conditionDiseaseRisk`) %>%
  mutate(time2_effect = b_conditionDiseaseRisk + `b_timeposttest2:conditionDiseaseRisk`) %>%
  gather(term, estimate, b_timeposttest2:time2_effect) %>%
  group_by(term) %>%
  summarize(p_above_zero = sum(ifelse(estimate > 0, 1, 0)) / n()) %>%
  filter(term == "time2_effect")
```

Using the marignal effects plot lets us pull out the model predictions without having to worry about interpreting beta regression coefficients. Here I'm using 50% credible intervals. It looks like, at immediate test, disease risk shows a significant improvement over control and autism correction, as previously observed. At delayed test, it's fallen back down but it looks like it's still expected to be better than control. I'll have to test that hypothesis inside the model somehow. Likewise, there's a hint that autism correction looks better than control in this sample/analysis, which I'll have to look at more closely.

```{r}
 
# fit_brm_ord_intent <- cbrm(
#   response ~ pretest + time * condition + (1 | workerId) + (1|item),
#   data = d2 %>%
#     filter(Scale == "vaxIntent") %>%
#     group_by(workerId, condition, phase) %>%
#     select(item, phase, condition, workerId, rating) %>%
#     spread(phase, rating) %>%
#     gather(time, response, posttest, posttest2),
#   family = cumulative(),
#   cached_file = "fit_brm_ord_intent.rds",
#   sample_prior = TRUE,
#   control = list(adapt_delta = .85),
#   cores = parallel::detectCores(),
#   iter = 2000
# )
```

```{r}
# summary(fit_brm_ord_intent)
# gg_marginal_effects(fit_brm_ord_intent, "time:condition") %>% plot()
```

### Predicting response

Now that we have multiple posttest periods, it might be preferrable to use a model that predicts responses across all phases. This feels a bit less piecemeal than the past models, although it might be less powerful.

Model:

```
response ~ phase * condition + (1|participant)
```


```{r}
fit_brm_beta_resp_intent <- cbrm(
  rescale_beta(response, 1, 6) ~ phase * condition + (1 | workerId),
  data = d2 %>%
    filter(Scale == "vaxIntent") %>%
    mutate(phase = relevel(factor(phase), ref = "pretest")) %>%
    group_by(workerId, condition, phase) %>%
    summarize(response = mean(rating)),
  family = Beta(),
  cached_file = "fit_brm_beta_resp_intent.rds",
  sample_prior = TRUE,
  control = list(adapt_delta = .95),
  cores = parallel::detectCores(),
  iter = 2000
)
```

```{r}
# summary(fit_brm_beta_resp_intent)
# gg_marginal_effects(fit_brm_beta_resp_intent, "phase:condition") %>% plot()
```

That seems like a pretty clear summary of the results, in a model that captures the pretest differences explicitly.

Just to be safe, we can do it as a hierarchical ordinal model as well. 

```{r}
fit_brm_ord2_resp_intent <- cbrm(
  rating ~ phase * condition + (1 | workerId) + (1 | item),
  data = d2 %>%
    filter(Scale == "vaxIntent") %>%
    group_by(workerId, condition, phase) %>%
    select(item, phase, condition, workerId, rating) %>%
    ungroup() %>%
    mutate(phase = relevel(factor(phase), ref = "pretest")),
  family = cumulative(),
  cached_file = "fit_brm_ord2_resp_intent.rds",
  sample_prior = TRUE,
  control = list(adapt_delta = .95),
  cores = parallel::detectCores(),
  iter = 2000
)
```

```{r}
# summary(fit_brm_ord2_resp_intent)
# gg_marginal_effects(fit_brm_ord2_resp_intent, "phase:condition") %>% plot()
```

So at the end of all that, it's not really all that clear what happened after a delay! The conclusions we'd draw seem to depend heavily on the analytic choices we make.


# IV. Additional analyses {.tabset}

To be explored:

- Examine demographics of participants
- Examine predictions of vaccine behavior for both scales (started)
- Estimate predicted reduction in vaccine refusals based on attitude change and behavior

## Vaccine behavior: delay and refusal 

How many parents have delayed, refused, or sought exemptions from vaccines? _Sought exemptio items should now be fixed, 4/9/18, 10:43 AM)_.


```{r}
d %>%
  group_by(parent, workerId) %>%
  summarize(
    flu = mean(flu, na.rm=TRUE),
    flu_next = mean(flu2, na.rm=TRUE),
    flu_child = mean(flu3, na.rm=TRUE),
    flu_child_next = mean(flu4, na.rm=TRUE),
    delay = mean(delay, na.rm=TRUE),
    refuse = mean(refuse, na.rm=TRUE)) %>%
  summarize(
    flu = mean(flu, na.rm=TRUE),
    flu_next = mean(flu_next, na.rm=TRUE),
    flu_child = mean(flu_child, na.rm=TRUE),
    flu_child_next = mean(flu_child_next, na.rm=TRUE),
    delay = mean(delay, na.rm=TRUE),
    refuse = mean(refuse, na.rm=TRUE))
```

How do pretest vaccine attitudes predict refusals?

```{r}
d_sum_behavior <- d %>%
  mutate(youngest = as.numeric(youngest)) %>%
  filter(Scale=="vaxAttitude") %>%
  mutate(young_parent = ifelse( ((parent == 1 & youngest < 12) | (parent == 0 & hope_par == 1)), 1, 0 )) %>%
  mutate(young_parent = ifelse(is.na(young_parent),0,young_parent)) %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating),
            young_parent = first(young_parent),
            refuse = first(refuse),
            delay = first(delay),
            flu = first(flu),
            flu_child = first(flu3)) %>%
  spread(phase, mean_rating) %>%
  mutate(change = posttest - pretest)

fit_refuse <- glm(refuse ~ pretest, data = d_sum_behavior, family="binomial")
summary(fit_refuse)
```

```{r}

# here's some sloppy code to plot that model

model <- glm(refuse ~ pretest,
  data = d_sum_behavior,
  family=binomial) #etc

temp.data <- data.frame(
  pretest =  seq(from=1, to = 6, length.out = 300))

temp.data <- cbind(temp.data, predict(model, temp.data, type = "link", se.fit=TRUE))

temp.data <- within(temp.data, {
  yHat <- model$family$linkinv(fit)
  LL <- model$family$linkinv(fit - 1.96 * se.fit)
  UL <- model$family$linkinv(fit + 1.96 * se.fit)
})

plt.refuse <- ggplot(temp.data, aes(x = pretest, y = yHat)) +
  geom_ribbon(aes(ymin = LL, ymax = UL), fill="grey", alpha = .4) +
  geom_line(colour = "blue") +
  labs(x = "Vaccine attitudes (pre-test)", y = "P(Refusal)", title="Child Vaccination refusals") +
  geom_jitter(data=d_sum_behavior, 
              aes(x=pretest, y=refuse), 
              inherit.aes = FALSE,
              height=.025,
              width=.05,
              alpha=.5,
              shape=1) +
  theme_minimal(base_size = 18) +
  theme(aspect.ratio = 1)

plt.refuse

```

## Parents vs non-parents

How many Ps are parents?

```{r}
d %>% 
  group_by(condition, workerId) %>%
  summarize(parent = first(parent)) %>%
  summarize(prop_parent = mean(parent))
  
```

Looks like approximately 44-45% of participants are parents. Let's see how the age of their youngest children breaks down.

```{r}

d <- d %>% 
  mutate(youngest = as.numeric(as.character(youngest)),
         oldest = as.numeric(as.character(oldest)),
         youngest = ifelse(num_child==1,oldest,youngest))
d %>%
  group_by(workerId) %>%
  summarize(youngest = mean(youngest)) %>%
  ggplot(aes(x=youngest)) +
  geom_histogram()
```

And how many are planning to become parents soon?

- 0 = no
- 1 = Yes, in next 1-2 years
- 2 = Yes, in next 2-5 years
- 3 = yes, but not for 5 or more years

```{r}
d %>%
  group_by(workerId) %>%
  summarize(hope_par = first(hope_par)) %>%
  ggplot(aes(x=hope_par)) +
  geom_histogram()
```

Here we can focus in on the primary demographic of our interventions, parents of young children and those who are soon to be parents. Below, we code as `young_parent` parents with children under 12 and people who hope to become parents within the next 2 years.

```{r}

d_sum_pars <- d %>%
  filter(Scale=="vaxAttitude") %>%
  mutate(young_parent = ifelse( ((parent == 1 & youngest < 12) | (parent == 0 & hope_par == 1)), 1, 0 )) %>%
  mutate(young_parent = ifelse(is.na(young_parent),0,young_parent)) %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating),
            young_parent = first(young_parent)) %>%
  spread(phase, mean_rating) %>%
  mutate(change = posttest - pretest)

fit_beta_pars <- betareg(rescale_beta(posttest,1,6) ~ scale(pretest) + condition*young_parent,
           data=d_sum_pars)

summary(fit_beta_pars)
```

No significant effects of parenthood nor interactions with condition effects.

## Effect of autism correction condition on "autism" item

All told, the Autism Correction condition did not affect overall vaccine attitudes. However, it did clearly reduce agreement with the added item "Some vaccines cause autism in healthy children" (as found originally). So, it's possible that people accept the information in the condiiton, but that it is only a very small part of what drives their overall attitudes toward vaccination.

```{r}
d %>%
  filter(Scale=="autism") %>%
  group_by(workerId, condition, phase) %>%
  summarize(mean_rating = mean(rating)) %>%
  spread(phase, mean_rating) %>%
  summarize(change_score = posttest-pretest) %>%
  group_by(condition) %>%
  summarize(mean_change = mean(change_score),
            ul = mean(change_score) + stderr(change_score),
            ll = mean(change_score) - stderr(change_score)) %>%
  ggplot(aes(x=condition, fill=condition, y = mean_change, ymin=ll, ymax=ul)) +
  geom_bar(stat="identity") +
  geom_errorbar(width=.25) +
  theme_minimal() +
  labs(title="Change Score Analysis: 'Autism' Item",
       y = "Mean Change Score",
       x = "Condition",
       fill = "Condition")
```


# Summary

This study successfully replicated the original findings of the _PNAS_ paper according to both its "direct replication" criterion and the "best model" criterion. The "disease risk" manipulation significantly improved attitudes toward vaccines as measured by the 5-item "vaccine attitudes" scale. The intervention also increased participants' reported intentions to vaccinate their children, as measured by our new 5-item "vaccine intentions" scale.

### Miscellany

A participante noticed a typo in the "confident" vaccine Intent item, it said "... in the right thing to do" when it should have said "... is the right thing to do".

### Replication notes

Analyses can be replicated using `derekpowell/rstudio-dmp:20180321` Docker image.

### `sessionInfo()`

```{r}
sessionInfo()
```

